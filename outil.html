<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Résultats de recherche</title>
    <link rel="stylesheet" href="main_styles.css">
</head>
<body>
    <div class="container">
        <header>
            <nav>
                <a href="index.html">Accueil</a>
                <a href="metho.html">Méthodologie</a>
                <a href="veille.html">CR de Veille</a>
                <a href="biblio.html">Bibliographie</a>
                <a href="outil.html">Outil de recherche</a>
            </nav>
        </header>
        
        <h2>Exemple d'utilisation de l'outil de recherche via les API avant une version de production de l'application</h2>
        <p>Le projet est disponnible sur mon GitHub : <a href="https://github.com/MaximeCerise/outil-recherche-veille">lien</a></p>
        <h1>Résultats pour : Reinforcement Learning</h1>
        
        <h2>Google Scholar</h2>
        <ul>
            <li><a href='https://www.jair.org/index.php/jair/article/view/10166'><strong>Reinforcement learning: A survey</strong></a><br><em>LP Kaelbling, ML Littman, AW Moore - Journal of artificial intelligence …, 1996 - jair.org</em><br><strong>Résumé :</strong>… This paper surveys the field of reinforcement learning from a … learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning …</li><li><a href='https://link.springer.com/content/pdf/10.1007/978-3-642-27645-3.pdf'><strong>Reinforcement learning</strong></a><br><em>MA Wiering, M Van Otterlo - Adaptation, learning, and optimization, 2012 - Springer</em><br><strong>Résumé :</strong>… This includes PhD and master students, researchers in reinforcement learning itself, and researchers in any other field who want to know about reinforcement learning. Having a …</li><li><a href='https://arxiv.org/abs/1701.07274'><strong>Deep reinforcement learning: An overview</strong></a><br><em>Y Li - arXiv preprint arXiv:1701.07274, 2017 - arxiv.org</em><br><strong>Résumé :</strong>… exciting achievements of deep reinforcement learning (RL). … machine learning, deep learning and reinforcement learning. … memory, unsupervised learning, transfer learning, multiagent …</li><li><a href='https://www.academia.edu/download/6985553/ivry_rev.pdf'><strong>Reinforcement learning</strong></a><br><em>RS Sutton, AG Barto - Journal of Cognitive Neuroscience, 1999 - academia.edu</em><br><strong>Résumé :</strong>… algorithms that learn a … reinforcement learning originated in psychological theorizing, in recent years these ideas have been most extensively developed within the artificial learning …</li><li><a href='https://www.sciencedirect.com/science/article/pii/B9780125264303500039'><strong>Reinforcement learning</strong></a><br><em>AG Barto - Neural systems for control, 1997 - Elsevier</em><br><strong>Résumé :</strong>… Reinforcement learning allows autonomous systems to learn from their experiences instead … reinforcement learning research directed toward developing capable artificial learning …</li><li><a href='https://books.google.com/books?hl=en&lr=&id=g4RyEAAAQBAJ&oi=fnd&pg=PP1&dq=Reinforcement+Learning&ots=wm6XUiWB2T&sig=0e3m8r2_sg3wFysiX0b98fnVy8c'><strong>Algorithms for reinforcement learning</strong></a><br><em>C Szepesvári - 2022 - books.google.com</em><br><strong>Résumé :</strong>… Reinforcement learning is of great interest … of reinforcement learning that build on the powerful theory of dynamic programming. We give a fairly comprehensive catalog of learning …</li><li><a href='https://ieeexplore.ieee.org/abstract/document/8103164/'><strong>Deep reinforcement learning: A brief survey</strong></a><br><em>K Arulkumaran, MP Deisenroth… - IEEE Signal …, 2017 - ieeexplore.ieee.org</em><br><strong>Résumé :</strong>… learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to … with their environments to learn optimal behaviors, improving …</li><li><a href='http://msl.cs.illinois.edu/~lavalle/cs397/kaelbling96reinforcement.ps'><strong>Reinforcement learning: A survey</strong></a><br><em>ML Littman, AW Moore - Journal of artificial intelligence …, 1996 - msl.cs.illinois.edu</em><br><strong>Résumé :</strong>… This paper surveys the ﬁeld of reinforcement learning from a … learning. Both the historical basis of the ﬁeld and a broad selection of current work are summarized. Reinforcement learning …</li><li><a href='https://www.cambridge.org/core/journals/robotica/article/robot-learning-edited-by-jonathan-h-connell-and-sridhar-mahadevan-kluwer-boston-19931997-xii240-pp-isbn-0792393651-hardback-21800-guilders-12000-8995/737FD21CA908246DF17779E9C20B6DF6'><strong>Reinforcement learning: An introduction</strong></a><br><em>RS Sutton, AG Barto - 1998 - cambridge.org</em><br><strong>Résumé :</strong>… the basis for powerful methods of reinforcement learning. It is … , and the reinforcement-learning principles can be adapted. … robotics applications of reinforcement learning are treated …</li><li><a href='https://arxiv.org/abs/1611.05763'><strong>Learning to reinforcement learn</strong></a><br><em>JX Wang, Z Kurth-Nelson, D Tirumala, H Soyer… - arXiv preprint arXiv …, 2016 - arxiv.org</em><br><strong>Résumé :</strong>… In recent years deep reinforcement learning (RL) systems have attained … meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a …</li>
        </ul>
        
    <h2>Semantic Scholar</h2>
        <ul>
            <li><a href='https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d'><strong>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</strong></a> - Tuomas Haarnoja, Aurick Zhou, P. Abbeel, S. Levine (2018)</li><strong>Résumé :</strong> Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.</li><li><a href='https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd'><strong>Asynchronous Methods for Deep Reinforcement Learning</strong></a> - Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, T. Lillicrap, Tim Harley, David Silver, K. Kavukcuoglu (2016)</li><strong>Résumé :</strong> We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.</li><li><a href='https://www.semanticscholar.org/paper/67d968c7450878190e45ac7886746de867bf673d'><strong>Neural Architecture Search with Reinforcement Learning</strong></a> - Barret Zoph, Quoc V. Le (2016)</li><strong>Résumé :</strong> Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.</li><li><a href='https://www.semanticscholar.org/paper/340f48901f72278f6bf78a04ee5b01df208cc508'><strong>Human-level control through deep reinforcement learning</strong></a> - Volodymyr Mnih, K. Kavukcuoglu, David Silver, Andrei A. Rusu, J. Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, A. Fidjeland, Georg Ostrovski, Stig Petersen, Charlie Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, D. Kumaran, D. Wierstra, S. Legg, D. Hassabis (2015)</li><strong>Résumé :</strong> None</li><li><a href='https://www.semanticscholar.org/paper/0286b2736a114198b25fb5553c671c33aed5d477'><strong>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</strong></a> - Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, T. Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, S. El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, C. Olah, Benjamin Mann, Jared Kaplan (2022)</li><strong>Résumé :</strong> We apply preference modeling and reinforcement learning from human feedback (RLHF) to ﬁnetune language models to act as helpful and harmless assistants. We ﬁnd this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efﬁciently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work. Figure These plots show that PM accuracy decreases as we focus exclusively on comparisons between pairs of samples with high score. We have normalized all preference models to have the same mean score on a held-out dataset so that they’re directly comparable, and then plotted accuracy for the comparisons where both samples have scores above a speciﬁc threshold.</li><li><a href='https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158'><strong>Playing Atari with Deep Reinforcement Learning</strong></a> - Volodymyr Mnih, K. Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, D. Wierstra, Martin A. Riedmiller (2013)</li><strong>Résumé :</strong> We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.</li><li><a href='https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f'><strong>Continuous control with deep reinforcement learning</strong></a> - T. Lillicrap, Jonathan J. Hunt, A. Pritzel, N. Heess, Tom Erez, Yuval Tassa, David Silver, D. Wierstra (2015)</li><strong>Résumé :</strong> We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.</li><li><a href='https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e'><strong>Deep Reinforcement Learning with Double Q-Learning</strong></a> - H. V. Hasselt, A. Guez, David Silver (2015)</li><strong>Résumé :</strong> 
 
 The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.
 
</li><li><a href='https://www.semanticscholar.org/paper/361c00b22e29d0816ca896513d2c165e26399821'><strong>Grandmaster level in StarCraft II using multi-agent reinforcement learning</strong></a> - O. Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David Choi, Richard Powell, T. Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, M. Kroiss, Ivo Danihelka, Aja Huang, L. Sifre, Trevor Cai, J. Agapiou, Max Jaderberg, A. Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, D. Budden, Yury Sulsky, James Molloy, T. Paine, Caglar Gulcehre, Ziyun Wang, T. Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith, T. Schaul, T. Lillicrap, K. Kavukcuoglu, D. Hassabis, C. Apps, David Silver (2019)</li><strong>Résumé :</strong> None</li><li><a href='https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054'><strong>Reinforcement Learning: An Introduction</strong></a> - R. S. Sutton, A. Barto (1998)</li><strong>Résumé :</strong> Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.</li>
        </ul>
        
        <h2>GitHub</h2>
        <ul>
            <li><a href='https://github.com/labmlai/annotated_deep_learning_paper_implementations'>annotated_deep_learning_paper_implementations</a> - ⭐ 59376</li><li><a href='https://github.com/openai/gym'>gym</a> - ⭐ 35650</li><li><a href='https://github.com/pytorch/examples'>examples</a> - ⭐ 22866</li><li><a href='https://github.com/dennybritz/reinforcement-learning'>reinforcement-learning</a> - ⭐ 21087</li><li><a href='https://github.com/Unity-Technologies/ml-agents'>ml-agents</a> - ⭐ 17852</li><li><a href='https://github.com/openai/baselines'>baselines</a> - ⭐ 16146</li><li><a href='https://github.com/ShangtongZhang/reinforcement-learning-an-introduction'>reinforcement-learning-an-introduction</a> - ⭐ 13940</li><li><a href='https://github.com/huggingface/trl'>trl</a> - ⭐ 12726</li><li><a href='https://github.com/kmario23/deep-learning-drizzle'>deep-learning-drizzle</a> - ⭐ 12523</li><li><a href='https://github.com/AI4Finance-Foundation/FinRL'>FinRL</a> - ⭐ 11229</li><li><a href='https://github.com/openai/spinningup'>spinningup</a> - ⭐ 10706</li><li><a href='https://github.com/google/dopamine'>dopamine</a> - ⭐ 10674</li><li><a href='https://github.com/DLR-RM/stable-baselines3'>stable-baselines3</a> - ⭐ 10171</li><li><a href='https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow'>Reinforcement-learning-with-tensorflow</a> - ⭐ 9117</li><li><a href='https://github.com/aikorea/awesome-rl'>awesome-rl</a> - ⭐ 9022</li><li><a href='https://github.com/Farama-Foundation/Gymnasium'>Gymnasium</a> - ⭐ 8662</li><li><a href='https://github.com/thu-ml/tianshou'>tianshou</a> - ⭐ 8317</li><li><a href='https://github.com/evilsocket/pwnagotchi'>pwnagotchi</a> - ⭐ 8177</li><li><a href='https://github.com/lucidrains/PaLM-rlhf-pytorch'>PaLM-rlhf-pytorch</a> - ⭐ 7768</li><li><a href='https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning'>Book-Mathematical-Foundation-of-Reinforcement-Learning</a> - ⭐ 7725</li><li><a href='https://github.com/tensorlayer/TensorLayer'>TensorLayer</a> - ⭐ 7345</li><li><a href='https://github.com/PWhiddy/PokemonRedExperiments'>PokemonRedExperiments</a> - ⭐ 7240</li><li><a href='https://github.com/yenchenlin/DeepLearningFlappyBird'>DeepLearningFlappyBird</a> - ⭐ 6716</li><li><a href='https://github.com/vwxyzjn/cleanrl'>cleanrl</a> - ⭐ 6599</li><li><a href='https://github.com/yandexdataschool/Practical_RL'>Practical_RL</a> - ⭐ 6075</li><li><a href='https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch'>Deep-Reinforcement-Learning-Algorithms-with-PyTorch</a> - ⭐ 5756</li><li><a href='https://github.com/keras-rl/keras-rl'>keras-rl</a> - ⭐ 5542</li><li><a href='https://github.com/volcengine/verl'>verl</a> - ⭐ 5494</li><li><a href='https://github.com/udacity/deep-reinforcement-learning'>deep-reinforcement-learning</a> - ⭐ 5010</li><li><a href='https://github.com/tensortrade-org/tensortrade'>tensortrade</a> - ⭐ 4899</li>
        </ul>
        <a href="/">Nouvelle recherche</a>
    </div>
</body>
</html>
